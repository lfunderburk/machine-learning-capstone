{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importance\n",
    "\n",
    "#### Feature selection\n",
    "\n",
    "TO DO: READ ABOUT RECURSIVE FEATURE ELIMINTATION!!!!\n",
    "\n",
    "Do recursive feature elimination first, then decision tree, get feature importance , set a threshold and choose the top 5 most important features\n",
    "\n",
    "\n",
    "L1 (L2) regularization\n",
    "\n",
    "PCA, then random forest classifier, get feature importance \n",
    "\n",
    "Autoencoder\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "KMEANS\n",
    "DBSCAN (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: clustering fuel only cars\n",
    "1. T-SNE, read about it, do dimensionality reduction\n",
    "\n",
    "## Pair wise clustering\n",
    "1. Hybrid and electric\n",
    "2. Hybrid and fuel\n",
    "\n",
    "## Fuzzy logic\n",
    "Can we calculate car membership?\n",
    "Read on fuzzy logic \n",
    "\n",
    "## Consumer analysis with scraped data \n",
    "Predictor for time series sales data\n",
    "Couple sales data with the co2 scores\n",
    "Accumulated prediction for co2 impact \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','./data/', './clean-data/')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..','./scripts/')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "clean_data = os.path.abspath(os.path.join(os.getcwd(), \"..\",'data', 'predicted-data'))\n",
    "\n",
    "fuel_df = pd.read_csv(Path(clean_data,\"predicted_co2_rating.csv\"))\n",
    "hybrid_df = pd.read_csv(Path(clean_data,\"predicted_co2_rating_hybrid.csv\"))\n",
    "electric_df = pd.read_csv(Path(clean_data,\"predicted_co2_rating_electric.csv\"))\n",
    "\n",
    "# drop the 'original_co2r' column from the dataframes\n",
    "fuel_df.drop('original_co2r', axis=1, inplace=True)\n",
    "\n",
    "# drop smog_rating from the dataframes\n",
    "hybrid_df.drop('smog_rating', axis=1, inplace=True)\n",
    "electric_df.drop('smog_rating', axis=1, inplace=True)\n",
    "\n",
    "# rename co2_rating to predicted_co2r in hybrid_df and electric_df\n",
    "hybrid_df.rename(columns={'co2_rating': 'predicted_co2_rating'}, inplace=True)\n",
    "electric_df.rename(columns={'co2_rating': 'predicted_co2_rating'}, inplace=True)\n",
    "\n",
    "# rename fuel_type1 to fuel_type in fuel_df, hybrid_df, and electric_df\n",
    "fuel_df.rename(columns={'fuel_type1': 'fuel_type'}, inplace=True)\n",
    "hybrid_df.rename(columns={'fuel_type1': 'fuel_type'}, inplace=True)\n",
    "electric_df.rename(columns={'fuel_type1': 'fuel_type'}, inplace=True)\n",
    "\n",
    "\n",
    "# merge vehicle_type and id into a single column for fuel_df\n",
    "\n",
    "def merge_vehicle_id(df):\n",
    "    df['vehicle_id'] = df['vehicle_type'] + '_' + df['id'].astype(str)\n",
    "\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "fuel_df = merge_vehicle_id(fuel_df)\n",
    "hybrid_df = merge_vehicle_id(hybrid_df)\n",
    "electric_df = merge_vehicle_id(electric_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fuel_df, hybrid_df, electric_df], axis=0, ignore_index=True)\n",
    "\n",
    "# move vehicle_id to the first column\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# obtain position of vehicle_id column from cols\n",
    "vehicle_id_index = cols.index('vehicle_id')\n",
    "\n",
    "# move vehicle_id to the first column\n",
    "cols = [cols[vehicle_id_index]] + cols[:vehicle_id_index] + cols[vehicle_id_index+1:]\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "# fill missing values with 0\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# create a correlation matrix\n",
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "# plot the correlation matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert f['vehicle_type'] to a categorical variable\n",
    "df['vehicle_type'] = df['vehicle_type'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCATTER PLOT OF co2_emissions_gpm vs. predicted_co2r\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y=df['co2emissions_(g/km)'], x=df['fuelconsumption_comb(mpg)'], c=df['vehicle_type'], cmap='coolwarm')\n",
    "plt.xlabel('co2emissions_(g/km)')\n",
    "plt.ylabel('model_year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns with numerical and categorical data\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "df_cat = pd.get_dummies(df[['vehicleclass_','make_','vehicle_type','transmission_type']])\n",
    "df_num = df[num_cols]\n",
    "\n",
    "# Combine the numerical and one-hot encoded categorical dataframes\n",
    "df_processed = pd.concat([df_num, df_cat], axis=1)\n",
    "\n",
    "# Perform recursive feature elimination to select the most important features\n",
    "X = df_processed.drop([\"predicted_co2_rating\"], axis=1)\n",
    "y = df_processed[\"predicted_co2_rating\"]\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rfecv = RFECV(estimator=dt, cv=10)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "\n",
    "# Print the most important features\n",
    "print(\"Selected features:\", X.columns[rfecv.support_])\n",
    "\n",
    "# Fit a decision tree and get feature importances\n",
    "dt.fit(X, y)\n",
    "feat_importances = pd.Series(dt.feature_importances_, index=X.columns)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure()\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "# Select the top 5 most important features\n",
    "threshold = feat_importances.nlargest(10).min()\n",
    "top_features = feat_importances[feat_importances >= threshold].index.tolist()\n",
    "print(\"Top features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected features:\", X.columns[rfecv.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree and get feature importances\n",
    "dt.fit(X, y)\n",
    "feat_importances = pd.Series(dt.feature_importances_, index=X.columns)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure()\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "# Select the top 5 most important features\n",
    "threshold = feat_importances.nlargest(10).min()\n",
    "top_features = feat_importances[feat_importances >= threshold].index.tolist()\n",
    "print(\"Top features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# select the top 5 important features\n",
    "# select only the top 5 important features\n",
    "X = X[top_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[top_features])\n",
    "\n",
    "# create a range of cluster numbers\n",
    "range_n_clusters = range(2, 21)\n",
    "\n",
    "# initialize lists to store the scores\n",
    "silhouette_scores = []\n",
    "inertia_scores = []\n",
    "\n",
    "# loop over the range of cluster numbers\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # initialize KMeans with n_clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    \n",
    "    # fit KMeans to the data\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # calculate the inertia score (within-cluster sum of squares)\n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "    \n",
    "    # calculate the silhouette score\n",
    "    silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # print the scores\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The inertia score is :\", kmeans.inertia_,\n",
    "          \"The average silhouette score is :\", silhouette_avg)\n",
    "\n",
    "# plot the elbow curve\n",
    "plt.plot(range_n_clusters, inertia_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia Score')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# plot the silhouette scores\n",
    "plt.plot(range_n_clusters, silhouette_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "\n",
    "# fit DBSCAN to the data\n",
    "dbscan.fit(X_scaled)\n",
    "\n",
    "# print the number of clusters (unique labels) in DBSCAN\n",
    "print(\"Number of clusters in DBSCAN:\", len(np.unique(dbscan.labels_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(n,df):\n",
    "\n",
    "    df_c = df.copy()\n",
    "    # k means\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0)\n",
    "    df_c['cluster'] = kmeans.fit_predict(df_c[['Attack', 'Defense']])\n",
    "    # get centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    cen_x = [i[0] for i in centroids] \n",
    "    cen_y = [i[1] for i in centroids]\n",
    "    ## add to df\n",
    "    df['cen_x'] = df.cluster.map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})\n",
    "    df['cen_y'] = df.cluster.map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})\n",
    "    # define and map colors\n",
    "    colors = ['#DF2020', '#81DF20', '#2095DF']\n",
    "    df['c'] = df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_dbscan(X, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    return n_clusters, labels\n",
    "\n",
    "def find_best_dbscan_params(X, eps_list, min_samples_list):\n",
    "    silhouette_scores = []\n",
    "    for eps in eps_list:\n",
    "        for min_samples in min_samples_list:\n",
    "            n_clusters, labels = perform_dbscan(X, eps, min_samples)\n",
    "            if n_clusters > 1:\n",
    "                silhouette_score = metrics.silhouette_score(X, labels)\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters,\n",
    "                      \"and the average silhouette score is\", silhouette_score)\n",
    "                silhouette_scores.append(silhouette_score)\n",
    "            else:\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters)\n",
    "    # print the parameter combination with the highest silhouette score\n",
    "    max_index = silhouette_scores.index(max(silhouette_scores))\n",
    "    eps_index = max_index // len(min_samples_list)\n",
    "    min_samples_index = max_index % len(min_samples_list)\n",
    "    best_eps = eps_list[eps_index]\n",
    "    best_min_samples = min_samples_list[min_samples_index]\n",
    "    print(\"The best parameter combination is eps =\", best_eps,\n",
    "          \"and min_samples =\", best_min_samples)\n",
    "    return best_eps, best_min_samples\n",
    "\n",
    "def perform_best_dbscan(X, best_eps, best_min_samples):\n",
    "    n_clusters, labels = perform_dbscan(X, best_eps, best_min_samples)\n",
    "    if n_clusters > 1:\n",
    "        silhouette_score = metrics.silhouette_score(X, labels)\n",
    "        print(\"Number of clusters in DBSCAN:\", n_clusters)\n",
    "        print(\"The average silhouette score is :\", silhouette_score)\n",
    "    else:\n",
    "        print(\"DBSCAN only found one cluster\")\n",
    "\n",
    "\n",
    "eps_list = [0.5, 1, 2, 4, 8, 16]\n",
    "min_samples_list = [5, 10, 20, 50, 100, 200]\n",
    "\n",
    "best_eps, best_min_samples = find_best_dbscan_params(X_scaled, eps_list, min_samples_list)\n",
    "perform_best_dbscan(X_scaled, best_eps, best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dbscan(X, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    return n_clusters, labels\n",
    "\n",
    "def find_best_dbscan_params(X, eps_list, min_samples_list):\n",
    "    max_silhouette_score = -1\n",
    "    best_eps, best_min_samples = None, None\n",
    "    for eps in eps_list:\n",
    "        for min_samples in min_samples_list:\n",
    "            n_clusters, labels = perform_dbscan(X, eps, min_samples)\n",
    "            if n_clusters == 2:\n",
    "                silhouette_score = metrics.silhouette_score(X, labels)\n",
    "                if silhouette_score > max_silhouette_score:\n",
    "                    max_silhouette_score = silhouette_score\n",
    "                    best_eps, best_min_samples = eps, min_samples\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters,\n",
    "                      \"and the average silhouette score is\", silhouette_score)\n",
    "            else:\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters)\n",
    "    # print the best hyperparameters\n",
    "    print(\"The best hyperparameters for 2 clusters are: eps =\", best_eps,\n",
    "          \"and min_samples =\", best_min_samples)\n",
    "    return best_eps, best_min_samples\n",
    "\n",
    "def perform_best_dbscan(X, best_eps, best_min_samples):\n",
    "    n_clusters, labels = perform_dbscan(X, best_eps, best_min_samples)\n",
    "    if n_clusters == 2:\n",
    "        silhouette_score = metrics.silhouette_score(X, labels)\n",
    "        print(\"Number of clusters in DBSCAN:\", n_clusters)\n",
    "        print(\"The average silhouette score is:\", silhouette_score)\n",
    "    else:\n",
    "        print(\"DBSCAN did not find 2 clusters\")\n",
    "\n",
    "# example usage\n",
    "\n",
    "eps_list = [0.5, 4, 8, 16]\n",
    "min_samples_list = [5, 10, 20, 50, 100, 200]\n",
    "\n",
    "best_eps, best_min_samples = find_best_dbscan_params(X_scaled, eps_list, min_samples_list)\n",
    "perform_best_dbscan(X_scaled, best_eps, best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters, labels = perform_dbscan(X_scaled, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = scaler.inverse_transform(X_scaled)\n",
    "\n",
    "pd.DataFrame(original_data, columns = top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster']==1]['predicted_co2_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster']==0]['predicted_co2_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the data using t-SNE\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, perplexity=30, learning_rate=200, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the data using t-SNE\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml-project-env-10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1b475f47bca8e2da48eea161630da42d1e1d9e1dc58db9742c2ed9d06304115"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
