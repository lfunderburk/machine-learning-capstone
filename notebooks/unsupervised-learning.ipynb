{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importance\n",
    "\n",
    "#### Feature selection\n",
    "\n",
    "TO DO: READ ABOUT RECURSIVE FEATURE ELIMINTATION!!!!\n",
    "\n",
    "Do recursive feature elimination first, then decision tree, get feature importance , set a threshold and choose the top 5 most important features\n",
    "\n",
    "\n",
    "L1 (L2) regularization\n",
    "\n",
    "PCA, then random forest classifier, get feature importance \n",
    "\n",
    "Autoencoder\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "KMEANS\n",
    "DBSCAN (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: clustering fuel only cars\n",
    "1. T-SNE, read about it, do dimensionality reduction\n",
    "\n",
    "## Pair wise clustering\n",
    "1. Hybrid and electric\n",
    "2. Hybrid and fuel\n",
    "\n",
    "## Fuzzy logic\n",
    "Can we calculate car membership?\n",
    "Read on fuzzy logic \n",
    "\n",
    "## Consumer analysis with scraped data \n",
    "Predictor for time series sales data\n",
    "Couple sales data with the co2 scores\n",
    "Accumulated prediction for co2 impact \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..','./data/', './clean-data/')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..','./scripts/')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(Path(\"../data/predicted-data/\",\"predicted_co2_rating.csv\"))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns with numerical data\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "df_num = df[num_cols]\n",
    "\n",
    "# Perform recursive feature elimination to select the most important features\n",
    "X = df_num.drop([\"predicted_co2_rating\",'original_co2r','id'], axis=1)\n",
    "y = df_num[\"predicted_co2_rating\"]\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rfecv = RFECV(estimator=dt, cv=10)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Plot the number of features vs. cross-validation scores\n",
    "plt.figure()\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation')\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "# Print the most important features\n",
    "print(\"Selected features:\", X.columns[rfecv.support_])\n",
    "\n",
    "# Fit a decision tree and get feature importances\n",
    "dt.fit(X, y)\n",
    "feat_importances = pd.Series(dt.feature_importances_, index=X.columns)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure()\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "# Select the top 5 most important features\n",
    "threshold = feat_importances.nlargest(5).min()\n",
    "top_features = feat_importances[feat_importances >= threshold].index.tolist()\n",
    "print(\"Top features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# select the top 5 important features\n",
    "# select only the top 5 important features\n",
    "X = X[[\n",
    "    'model_year', 'cylinders_', 'fuelconsumption_city(l/100km)',\n",
    "       'fuelconsumption_hwy(l/100km)', 'co2emissions_(g/km)',\n",
    "       'number_of_gears', 'fuelconsumption_comb(mpg)'\n",
    "]]\n",
    "\n",
    "# standardize the data\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# create a range of cluster numbers\n",
    "range_n_clusters = range(2, 11)\n",
    "\n",
    "# initialize lists to store the scores\n",
    "silhouette_scores = []\n",
    "inertia_scores = []\n",
    "\n",
    "# loop over the range of cluster numbers\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # initialize KMeans with n_clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    \n",
    "    # fit KMeans to the data\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # calculate the inertia score (within-cluster sum of squares)\n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "    \n",
    "    # calculate the silhouette score\n",
    "    silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # print the scores\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The inertia score is :\", kmeans.inertia_,\n",
    "          \"The average silhouette score is :\", silhouette_avg)\n",
    "\n",
    "# plot the elbow curve\n",
    "plt.plot(range_n_clusters, inertia_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia Score')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# plot the silhouette scores\n",
    "plt.plot(range_n_clusters, silhouette_scores, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "\n",
    "# fit DBSCAN to the data\n",
    "dbscan.fit(X)\n",
    "\n",
    "# print the number of clusters (unique labels) in DBSCAN\n",
    "print(\"Number of clusters in DBSCAN:\", len(np.unique(dbscan.labels_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_dbscan(X, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    return n_clusters, labels\n",
    "\n",
    "def find_best_dbscan_params(X, eps_list, min_samples_list):\n",
    "    silhouette_scores = []\n",
    "    for eps in eps_list:\n",
    "        for min_samples in min_samples_list:\n",
    "            n_clusters, labels = perform_dbscan(X, eps, min_samples)\n",
    "            if n_clusters > 1:\n",
    "                silhouette_score = metrics.silhouette_score(X, labels)\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters,\n",
    "                      \"and the average silhouette score is\", silhouette_score)\n",
    "                silhouette_scores.append(silhouette_score)\n",
    "            else:\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters)\n",
    "    # print the parameter combination with the highest silhouette score\n",
    "    max_index = silhouette_scores.index(max(silhouette_scores))\n",
    "    eps_index = max_index // len(min_samples_list)\n",
    "    min_samples_index = max_index % len(min_samples_list)\n",
    "    best_eps = eps_list[eps_index]\n",
    "    best_min_samples = min_samples_list[min_samples_index]\n",
    "    print(\"The best parameter combination is eps =\", best_eps,\n",
    "          \"and min_samples =\", best_min_samples)\n",
    "    return best_eps, best_min_samples\n",
    "\n",
    "def perform_best_dbscan(X, best_eps, best_min_samples):\n",
    "    n_clusters, labels = perform_dbscan(X, best_eps, best_min_samples)\n",
    "    if n_clusters > 1:\n",
    "        silhouette_score = metrics.silhouette_score(X, labels)\n",
    "        print(\"Number of clusters in DBSCAN:\", n_clusters)\n",
    "        print(\"The average silhouette score is :\", silhouette_score)\n",
    "    else:\n",
    "        print(\"DBSCAN only found one cluster\")\n",
    "\n",
    "\n",
    "eps_list = [0.5, 4, 8, 16]\n",
    "min_samples_list = [5, 10, 20, 50, 100, 200]\n",
    "\n",
    "best_eps, best_min_samples = find_best_dbscan_params(X, eps_list, min_samples_list)\n",
    "perform_best_dbscan(X, best_eps, best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dbscan(X, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    return n_clusters, labels\n",
    "\n",
    "def find_best_dbscan_params(X, eps_list, min_samples_list):\n",
    "    max_silhouette_score = -1\n",
    "    best_eps, best_min_samples = None, None\n",
    "    for eps in eps_list:\n",
    "        for min_samples in min_samples_list:\n",
    "            n_clusters, labels = perform_dbscan(X, eps, min_samples)\n",
    "            if n_clusters == 2:\n",
    "                silhouette_score = metrics.silhouette_score(X, labels)\n",
    "                if silhouette_score > max_silhouette_score:\n",
    "                    max_silhouette_score = silhouette_score\n",
    "                    best_eps, best_min_samples = eps, min_samples\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters,\n",
    "                      \"and the average silhouette score is\", silhouette_score)\n",
    "            else:\n",
    "                print(\"For eps =\", eps, \"and min_samples =\", min_samples,\n",
    "                      \"the number of clusters is\", n_clusters)\n",
    "    # print the best hyperparameters\n",
    "    print(\"The best hyperparameters for 2 clusters are: eps =\", best_eps,\n",
    "          \"and min_samples =\", best_min_samples)\n",
    "    return best_eps, best_min_samples\n",
    "\n",
    "def perform_best_dbscan(X, best_eps, best_min_samples):\n",
    "    n_clusters, labels = perform_dbscan(X, best_eps, best_min_samples)\n",
    "    if n_clusters == 2:\n",
    "        silhouette_score = metrics.silhouette_score(X, labels)\n",
    "        print(\"Number of clusters in DBSCAN:\", n_clusters)\n",
    "        print(\"The average silhouette score is:\", silhouette_score)\n",
    "    else:\n",
    "        print(\"DBSCAN did not find 2 clusters\")\n",
    "\n",
    "# example usage\n",
    "X = df[['model_year', 'cylinders_', 'fuelconsumption_city(l/100km)',\n",
    "       'fuelconsumption_hwy(l/100km)', 'co2emissions_(g/km)',\n",
    "       'number_of_gears', 'fuelconsumption_comb(mpg)']].values\n",
    "eps_list = [0.5, 4, 8, 16]\n",
    "min_samples_list = [5, 10, 20, 50, 100, 200]\n",
    "\n",
    "best_eps, best_min_samples = find_best_dbscan_params(X, eps_list, min_samples_list)\n",
    "perform_best_dbscan(X, best_eps, best_min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert X to a Pandas DataFrame for easier plotting\n",
    "df = pd.DataFrame(X, columns=['model_year', 'cylinders_', 'fuelconsumption_city(l/100km)',\n",
    "       'fuelconsumption_hwy(l/100km)', 'co2emissions_(g/km)',\n",
    "       'number_of_gears', 'fuelconsumption_comb(mpg)'])\n",
    "\n",
    "# Use Seaborn's pairplot function to plot a scatter matrix\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the data using t-SNE\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml-project-env-10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1b475f47bca8e2da48eea161630da42d1e1d9e1dc58db9742c2ed9d06304115"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
